{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucfEBaJHlF3C",
        "outputId": "1bb3c31f-29fe-4d36-c24b-5bf8d1ab9a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thumbs_up image downloaded successfully.\n",
            "Thumbs_up image downloaded successfully.\n",
            "Thumbs_up image downloaded successfully.\n",
            "Thumbs_up image downloaded successfully.\n",
            "Thumbs_up image downloaded successfully.\n",
            "Thumbs_down image downloaded successfully.\n",
            "Thumbs_down image downloaded successfully.\n",
            "Thumbs_down image downloaded successfully.\n",
            "Thumbs_down image downloaded successfully.\n",
            "Thumbs_down image downloaded successfully.\n",
            "Peace image downloaded successfully.\n",
            "Peace image downloaded successfully.\n",
            "Peace image downloaded successfully.\n",
            "Peace image downloaded successfully.\n",
            "Peace image downloaded successfully.\n",
            "Fist image downloaded successfully.\n",
            "Fist image downloaded successfully.\n",
            "Fist image downloaded successfully.\n",
            "Fist image downloaded successfully.\n",
            "Fist image downloaded successfully.\n",
            "Okay image downloaded successfully.\n",
            "Okay image downloaded successfully.\n",
            "Okay image downloaded successfully.\n",
            "Okay image downloaded successfully.\n",
            "Okay image downloaded successfully.\n",
            "Stop image downloaded successfully.\n",
            "Stop image downloaded successfully.\n",
            "Stop image downloaded successfully.\n",
            "Stop image downloaded successfully.\n",
            "Stop image downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 0.1083 - loss: 1.9145\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.2167 - loss: 1.8157\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.2286 - loss: 1.8205\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.1825 - loss: 1.8395\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.2389 - loss: 1.7800\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.1810 - loss: 1.7792\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - accuracy: 0.1349 - loss: 1.8092\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.2286 - loss: 1.7519\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.1083 - loss: 1.7607\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.2270 - loss: 1.7771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "# Create a directory to save gesture images\n",
        "if not os.path.exists('gesture_images'):\n",
        "    os.makedirs('gesture_images')\n",
        "\n",
        "# List of hand gestures to scrape images for\n",
        "gestures = ['thumbs_up', 'thumbs_down', 'peace', 'fist', 'okay', 'stop']\n",
        "\n",
        "# Function to download images from Google\n",
        "def download_gesture_images(gesture, num_images=5):\n",
        "    url = f\"https://www.google.com/search?q={gesture}+gesture&tbm=isch\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    images = soup.find_all('img')\n",
        "    count = 0\n",
        "\n",
        "    gesture_dir = f'gesture_images/{gesture}'\n",
        "    if not os.path.exists(gesture_dir):\n",
        "        os.makedirs(gesture_dir)\n",
        "\n",
        "    for img in images:\n",
        "        if count >= num_images:\n",
        "            break\n",
        "\n",
        "        img_url = img['src']\n",
        "        try:\n",
        "            img_data = requests.get(img_url).content\n",
        "            img_name = f\"{gesture_dir}/{gesture}_{count}.jpg\"\n",
        "            with open(img_name, 'wb') as handler:\n",
        "                handler.write(img_data)\n",
        "            print(f\"{gesture.capitalize()} image downloaded successfully.\")\n",
        "            count += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Download 5 images per gesture (you can increase this)\n",
        "for gesture in gestures:\n",
        "    download_gesture_images(gesture)\n",
        "\n",
        "# Resize images and preprocess them\n",
        "img_size = (64, 64)  # Resize to 64x64 pixels\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=img_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize the image\n",
        "    return img_array\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i, gesture in enumerate(gestures):\n",
        "    gesture_dir = f'gesture_images/{gesture}'\n",
        "    for img_name in os.listdir(gesture_dir):\n",
        "        img_path = os.path.join(gesture_dir, img_name)\n",
        "        img_array = preprocess_image(img_path)\n",
        "        images.append(img_array)\n",
        "        labels.append(i)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "labels = to_categorical(labels, num_classes=len(gestures))\n",
        "\n",
        "# Define image augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten and fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(gestures), activation='softmax'))  # Output layer for classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "model.fit(datagen.flow(images, labels, batch_size=16), epochs=10)\n",
        "\n",
        "# Save the model\n",
        "model.save('gesture_recognition_model.h5')\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('gesture_recognition_model.h5')\n",
        "\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame for prediction\n",
        "    img = cv2.resize(frame, (64, 64))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Predict the gesture\n",
        "    prediction = model.predict(img)\n",
        "    predicted_gesture = gestures[np.argmax(prediction)]\n",
        "\n",
        "    # Draw an animated overlay\n",
        "    overlay = frame.copy()\n",
        "    cv2.putText(overlay, predicted_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    alpha = 0.5\n",
        "    frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
        "\n",
        "    # Display the prediction\n",
        "    cv2.imshow('Gesture Recognition', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ]
}